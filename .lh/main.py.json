{
    "sourceFile": "main.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 1,
            "patches": [
                {
                    "date": 1747831128903,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1747851675576,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -92,8 +92,17 @@\n     # Call OpenAI or Replicate here\r\n     return f\"https://fake.url/image-for-{prompt.replace(' ', '-')}.png\"\r\n \r\n \r\n+import asyncio\r\n+from langchain_community.agent_toolkits import PlayWrightBrowserToolkit\r\n+from playwright.async_api import async_playwright\r\n+# This should be inside an async context or loop\r\n+async def init_playwright_tools():\r\n+    p = await async_playwright().start()\r\n+    browser = await p.chromium.launch(headless=True)\r\n+    toolkit = PlayWrightBrowserToolkit.from_browser(async_browser=browser)\r\n+    return toolkit.get_tools()\r\n \r\n \r\n \r\n \r\n"
                }
            ],
            "date": 1747831128903,
            "name": "Commit-0",
            "content": "from typing import Annotated\r\n\r\nfrom langchain_community.tools.tavily_search import TavilySearchResults\r\nfrom langchain_core.messages import BaseMessage\r\nfrom typing_extensions import TypedDict\r\n\r\nfrom langgraph.graph import StateGraph, START, END\r\nfrom langgraph.graph.message import add_messages\r\nfrom langgraph.prebuilt import ToolNode, tools_condition\r\nimport os\r\nfrom langchain_groq import ChatGroq\r\nfrom langgraph.checkpoint.memory import MemorySaver\r\nfrom dotenv import load_dotenv\r\nload_dotenv()\r\nimport openai\r\nfrom io import BytesIO\r\nfrom langchain.tools import tool\r\nimport requests\r\n\r\n\r\nnews_tool = TavilySearchResults(max_results=2)\r\n\r\ntools = [news_tool]\r\n\r\n\r\nfrom langchain_core.messages import SystemMessage\r\n\r\nSYSTEM_PROMPT = (\r\n    \"You are an AI assistant tasked with helping users create engaging and shareable social media posts. \"\r\n    \"Leverage the available tools to gather relevant and accurate news or information based on the user's request. \"\r\n    \"Ensure the generated post is concise, creative, and tailored to the user's needs, while maintaining a friendly and professional tone.\"\r\n    \"Use Emojis and Hashtags to make the post more engaging.\"\r\n    \"generate atleast 500 words of post related content.\"\r\n    \"dont generate the post directly, instead ask the user if they want to generate the post or not.\"\r\n    \"You can only call one tool per turn. If the user asks for multiple things, choose the most relevant or ask them to narrow it down.\"\r\n)\r\n\r\n\r\n\r\nllm = ChatGroq(model=\"llama-3.1-8b-instant\")\r\n\r\n\r\nclass State(TypedDict):\r\n    messages: Annotated[list, add_messages]\r\n\r\n\r\nllm_with_tools = llm.bind_tools(tools)\r\n\r\n\r\n\r\nconfig = {\r\n    \"configurable\": {\r\n        \"thread_id\": \"1\",\r\n        \"system_message\": SYSTEM_PROMPT\r\n    }\r\n}\r\n\r\ndef chatbot(state: State):\r\n    system_msg = config[\"configurable\"].get(\"system_message\", \"\")\r\n    full_messages = [SystemMessage(content=system_msg), *state[\"messages\"]]\r\n    return {\"messages\": [llm_with_tools.invoke(full_messages)]}\r\n\r\n\r\n\r\ndef generate_and_save_image(prompt, image_path):\r\n    \"\"\"Generates an image using the OpenAI API and saves it to the specified path.\"\"\"\r\n    try:\r\n        response = openai.images.generate(\r\n            model=\"dall-e-3\",\r\n            prompt=prompt,\r\n            n=1,\r\n            size=\"1024x1024\",\r\n            response_format=\"url\"\r\n        )\r\n        image_url = response.data[0].url\r\n\r\n        # Download and save the image\r\n        image_data = requests.get(image_url).content\r\n        img = Image.open(BytesIO(image_data))\r\n        img.save(image_path)\r\n\r\n        print(f\"Image saved successfully at {image_path}\")\r\n    except Exception as e:\r\n        print(f\"Error generating or saving image: {e}\")\r\n\r\n\r\n\r\n\r\n@tool\r\ndef ImageGenerator(prompt: str) -> str:\r\n    \"\"\"Generates an image from a text prompt using DALL·E or another model.\"\"\"\r\n    # Call OpenAI or Replicate here\r\n    return f\"https://fake.url/image-for-{prompt.replace(' ', '-')}.png\"\r\n\r\n\r\n\r\n\r\n\r\n\r\ngraph_builder = StateGraph(State)\r\n\r\ngraph_builder.add_node(\"chatbot\", chatbot)\r\n\r\ngraph_builder.add_node(\"ImageGenerator\", generate_and_save_image)\r\n\r\n\r\n\r\ntool_node = ToolNode(tools=tools)\r\ngraph_builder.add_node(\"tools\", tool_node)\r\n\r\n\r\n\r\ngraph_builder.add_conditional_edges(\r\n    \"chatbot\",\r\n    tools_condition,\r\n)\r\ngraph_builder.add_edge(\"tools\", \"chatbot\")\r\n\r\ngraph_builder.add_edge(\"ImageGenerator\", END)\r\ngraph_builder.add_edge(START, \"chatbot\")\r\n\r\n\r\nmemory = MemorySaver()\r\ngraph = graph_builder.compile(checkpointer=memory)\r\n\r\nfrom IPython.display import Image, display\r\n\r\ntry:\r\n    display(Image(graph.get_graph().draw_mermaid_png()))\r\n    with open(\"graph_image.png\", \"wb\") as file:\r\n        file.write(graph.get_graph().draw_mermaid_png())\r\n\r\nexcept Exception:\r\n    pass\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\ndef stream_graph_updates(user_input: str, config: dict = {}):\r\n    events = graph.stream(\r\n        {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\r\n        config,\r\n        stream_mode=\"values\",\r\n    )\r\n    for event in events:\r\n        # each event is a dict like {\"messages\": [message_dicts]}\r\n        for message in event.get(\"messages\", []):\r\n            if hasattr(message, \"pretty_print\"):\r\n                message.pretty_print()\r\n                \r\n            else:\r\n                print(\"Assistant:\", message.get(\"content\", \"[No content]\"))\r\n\r\n\r\n\r\nwhile True:\r\n    try:\r\n        user_input = input(\"User: \")\r\n        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\r\n            print(\"Goodbye!\")\r\n            break\r\n        # if user_input.lower().startswith(\"/setprompt\"):\r\n        #     new_prompt = user_input[len(\"/setprompt\"):].strip()\r\n        #     config[\"configurable\"][\"system_message\"] = new_prompt\r\n        #     print(\"✅ System prompt updated.\")\r\n        #     continue\r\n        stream_graph_updates(user_input, config=config)\r\n    except Exception as e:\r\n        print(\"⚠️ Error:\", str(e))\r\n        break\r\n"
        }
    ]
}